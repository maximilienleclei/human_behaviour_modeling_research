# Repository Structure

Complete tree of all classes, methods, and functions (like `tree` command).

```
human_behaviour_modeling_research/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ device.py
â”‚   â”‚   â””â”€â”€ âš™ï¸  set_device(gpu_index) â†’ Set the global DEVICE variable
â”‚   â”œâ”€â”€ experiments.py
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  DataConfig â†’ Data configuration
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  ExperimentConfig â†’ Complete experiment configuration
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  ModelConfig â†’ Model architecture configuration
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  OptimizerConfig â†’ Optimizer configuration
â”‚   â”‚   â””â”€â”€ ğŸ›ï¸  TrainingConfig â†’ Training configuration
â”‚   â”œâ”€â”€ paths.py
â”‚   â””â”€â”€ state.py
â”‚       â””â”€â”€ ğŸ›ï¸  StatePersistenceConfig â†’ Configuration for state persistence and transfer modes in continual learning
â”œâ”€â”€ data/hf_control_tasks/
â”‚   â””â”€â”€ loaders.py
â”‚       â”œâ”€â”€ âš™ï¸  load_cartpole_data() â†’ Load CartPole-v1 dataset from HuggingFace
â”‚       â””â”€â”€ âš™ï¸  load_lunarlander_data() â†’ Load LunarLander-v2 dataset from HuggingFace
â”œâ”€â”€ data/simexp_control_tasks/source/
â”‚   â”œâ”€â”€ collect.py
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  PausedSeededWrapper
â”‚   â”‚   â”‚   â””â”€â”€ reset()
â”‚   â”‚   â””â”€â”€ âš™ï¸  save_data_callback(obs_t, obs_tp1, action, rew, terminated, truncated, info)
â”‚   â”œâ”€â”€ data_analysis.py
â”‚   â”‚   â””â”€â”€ âš™ï¸  parse_analyze_and_plot(filenames)
â”‚   â””â”€â”€ replay.py
â”œâ”€â”€ data/simexp_control_tasks/
â”‚   â”œâ”€â”€ environments.py
â”‚   â”‚   â””â”€â”€ âš™ï¸  get_data_file(env_name, subject) â†’ Get data filename for environment and subject
â”‚   â”œâ”€â”€ loaders.py
â”‚   â”‚   â””â”€â”€ âš™ï¸  load_human_data(env_name, use_cl_info, subject, holdout_pct) â†’ Load human behavior data from JSON files with random run holdout
â”‚   â””â”€â”€ preprocessing.py
â”‚       â”œâ”€â”€ ğŸ›ï¸  EpisodeDataset â†’ Dataset that returns complete episodes instead of individual steps
â”‚       â”œâ”€â”€ âš™ï¸  compute_session_run_ids(timestamps) â†’ Compute session and run IDs from episode timestamps
â”‚       â”œâ”€â”€ âš™ï¸  episode_collate_fn(batch) â†’ Collate episodes with padding to handle variable lengths
â”‚       â””â”€â”€ âš™ï¸  normalize_session_run_features(session_ids, run_ids) â†’ Normalize session and run IDs to [-1, 1] range
â”œâ”€â”€ dl/models/
â”‚   â”œâ”€â”€ dynamic.py
â”‚   â”‚   â””â”€â”€ ğŸ›ï¸  DynamicNetPopulation â†’ Wrapper for common/dynamic_net population with BatchedPopulation interface
â”‚   â”‚       â”œâ”€â”€ forward_batch(observations) â†’ Batched forward pass for all networks
â”‚   â”‚       â”œâ”€â”€ evaluate(observations, actions) â†’ Evaluate fitness (cross-entropy) of all networks
â”‚   â”‚       â”œâ”€â”€ mutate() â†’ Apply mutations to all networks in population
â”‚   â”‚       â”œâ”€â”€ select_simple_ga(fitness) â†’ Simple GA selection: top 50% survive and duplicate
â”‚   â”‚       â”œâ”€â”€ get_state_dict() â†’ Get state dict for checkpointing
â”‚   â”‚       â””â”€â”€ load_state_dict(state) â†’ Load state dict from checkpoint
â”‚   â”œâ”€â”€ feedforward.py
â”‚   â”‚   â””â”€â”€ ğŸ›ï¸  MLP â†’ Two-layer MLP with tanh activations
â”‚   â”‚       â”œâ”€â”€ forward(x) â†’ Forward pass returning logits
â”‚   â”‚       â””â”€â”€ get_probs(x) â†’ Get probability distribution over actions
â”‚   â””â”€â”€ recurrent.py
â”‚       â”œâ”€â”€ ğŸ›ï¸  RecurrentMLPReservoir â†’ Recurrent MLP with frozen reservoir (echo state network style)
â”‚       â”‚   â”œâ”€â”€ forward_step(x, h) â†’ Single timestep forward pass
â”‚       â”‚   â”œâ”€â”€ forward(x, h_0) â†’ Sequence forward pass
â”‚       â”‚   â””â”€â”€ get_probs(x, h) â†’ Get probability distribution over actions for a single step
â”‚       â””â”€â”€ ğŸ›ï¸  RecurrentMLPTrainable â†’ Recurrent MLP with trainable recurrent weights (rank-1 factorization)
â”‚           â”œâ”€â”€ forward_step(x, h) â†’ Single timestep forward pass
â”‚           â”œâ”€â”€ forward(x, h_0) â†’ Sequence forward pass
â”‚           â””â”€â”€ get_probs(x, h) â†’ Get probability distribution over actions for a single step
â”œâ”€â”€ dl/optim/
â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  create_episode_list(observations, actions, episode_boundaries) â†’ Convert flat data with episode boundaries into list of episode dicts
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  load_checkpoint(checkpoint_path) â†’ Load checkpoint from disk
â”‚   â”‚   â””â”€â”€ âš™ï¸  save_checkpoint(checkpoint_path, checkpoint_data) â†’ Save checkpoint to disk
â”‚   â””â”€â”€ sgd.py
â”‚       â””â”€â”€ âš™ï¸  optimize_sgd(model, optim_obs, optim_act, test_obs, test_act, output_size, metadata, checkpoint_path, max_optim_time, batch_size, learning_rate, loss_eval_interval_seconds, logger) â†’ Optimize model using SGD with backpropagation
â”œâ”€â”€ metrics/
â”‚   â”œâ”€â”€ comparison.py
â”‚   â”‚   â””â”€â”€ âš™ï¸  evaluate_progression_recurrent(model, episode_details, env, max_steps, use_cl_features) â†’ Quick evaluation to track progression during optimization (for recurrent models)
â”‚   â””â”€â”€ metrics.py
â”‚       â”œâ”€â”€ âš™ï¸  compute_cross_entropy(model, observations, actions) â†’ Compute cross-entropy loss
â”‚       â””â”€â”€ âš™ï¸  compute_macro_f1(model, observations, actions, num_samples, num_classes) â†’ Compute macro F1 score with multiple sampling trials
â”œâ”€â”€ ne/net/dynamic/
â”‚   â”œâ”€â”€ compute_test.py
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  WelfordRunningStandardizer
â”‚   â”‚   â””â”€â”€ âš™ï¸  barebone_run(verbose) â†’ Simple working example to demonstrate how to run computation for the
â”‚   â”œâ”€â”€ evolution.py
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  Net â†’ Network that expands/contracts through architectural mutations
â”‚   â”‚   â”‚   â”œâ”€â”€ initialize_architecture()
â”‚   â”‚   â”‚   â”œâ”€â”€ grow_node(in_node_1, role) â†’ Method first called during initialization to grow the irremovable
â”‚   â”‚   â”‚   â”œâ”€â”€ grow_connection(in_node, out_node)
â”‚   â”‚   â”‚   â”œâ”€â”€ prune_node(node_being_pruned) â†’ Removes an existing hidden node
â”‚   â”‚   â”‚   â”œâ”€â”€ prune_connection(in_node, out_node, node_being_pruned) â†’ Called by `prune_node` to remove the `node_being_pruned`'s
â”‚   â”‚   â”‚   â”œâ”€â”€ mutate()
â”‚   â”‚   â”‚   â”œâ”€â”€ clone() â†’ Create a deep copy of this network
â”‚   â”‚   â”‚   â”œâ”€â”€ get_state_dict() â†’ Serialize complete network state for checkpointing
â”‚   â”‚   â”‚   â””â”€â”€ load_state_dict(state) â†’ Restore network from serialized state
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  Node
â”‚   â”‚   â”‚   â”œâ”€â”€ sample_nearby_node(nodes_considered, local_connectivity_probability)
â”‚   â”‚   â”‚   â”œâ”€â”€ connect_to(node)
â”‚   â”‚   â”‚   â””â”€â”€ disconnect_from(node)
â”‚   â”‚   â””â”€â”€ ğŸ›ï¸  NodeList â†’ Holds `Node` instances for ease of manipulation
â”‚   â””â”€â”€ main.py
â”‚       â””â”€â”€ ğŸ›ï¸  DynamicNetPopulation â†’ Wrapper for dynamic network population with batched population interface
â”‚           â”œâ”€â”€ forward_batch(observations) â†’ Batched forward pass for all networks
â”‚           â”œâ”€â”€ evaluate(observations, actions) â†’ Evaluate fitness (cross-entropy) of all networks
â”‚           â”œâ”€â”€ mutate() â†’ Apply mutations to all networks in population
â”‚           â”œâ”€â”€ select_and_duplicate(fitness) â†’ Select top performers and duplicate to fill population
â”‚           â”œâ”€â”€ get_state_dict() â†’ Get state dict for checkpointing
â”‚           â””â”€â”€ load_state_dict(state) â†’ Load state dict from checkpoint
â”œâ”€â”€ ne/net/
â”‚   â”œâ”€â”€ feedforward.py
â”‚   â”‚   â””â”€â”€ ğŸ›ï¸  BatchedFeedforward â†’ Batched population of feedforward MLPs for efficient GPU-parallel computation
â”‚   â”‚       â”œâ”€â”€ forward_batch(x) â†’ Batched forward pass for all networks in parallel
â”‚   â”‚       â”œâ”€â”€ mutate() â†’ Apply mutations to all networks in parallel using adaptive or fixed sigma
â”‚   â”‚       â”œâ”€â”€ get_state_dict() â†’ Get network state for checkpointing
â”‚   â”‚       â”œâ”€â”€ load_state_dict(state) â†’ Restore network state from checkpoint
â”‚   â”‚       â”œâ”€â”€ get_parameters_flat() â†’ Get flattened parameter vectors for all networks
â”‚   â”‚       â”œâ”€â”€ set_parameters_flat(flat_params) â†’ Set network parameters from flat vectors
â”‚   â”‚       â””â”€â”€ clone_network(indices) â†’ Clone networks at specified indices to fill population
â”‚   â”œâ”€â”€ protocol.py
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  NetworkProtocol â†’ Base protocol that all network types must implement
â”‚   â”‚   â”‚   â”œâ”€â”€ forward_batch(x) â†’ Batched forward pass for all networks in parallel
â”‚   â”‚   â”‚   â”œâ”€â”€ mutate() â†’ Apply mutations to all networks in the population
â”‚   â”‚   â”‚   â”œâ”€â”€ get_state_dict() â†’ Get state dictionary for checkpointing
â”‚   â”‚   â”‚   â””â”€â”€ load_state_dict(state) â†’ Restore network from checkpoint state
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  ParameterizableNetwork â†’ Networks with flat parameter operations (for ES/CMA-ES optimizers)
â”‚   â”‚   â”‚   â”œâ”€â”€ get_parameters_flat() â†’ Get flattened parameter vectors for all networks
â”‚   â”‚   â”‚   â”œâ”€â”€ set_parameters_flat(params) â†’ Set network parameters from flat vectors
â”‚   â”‚   â”‚   â””â”€â”€ clone_network(indices) â†’ Clone networks at specified indices to fill population
â”‚   â”‚   â””â”€â”€ ğŸ›ï¸  StructuralNetwork â†’ Networks with evolving topology (for GA-only optimization)
â”‚   â”‚       â””â”€â”€ select_and_duplicate(fitness) â†’ Select top performers and duplicate to fill population
â”‚   â””â”€â”€ recurrent.py
â”‚       â””â”€â”€ ğŸ›ï¸  BatchedRecurrent â†’ Batched population of stacked recurrent MLPs for efficient GPU-parallel computation
â”‚           â”œâ”€â”€ forward_batch_step(x, h_states) â†’ Single timestep forward pass for all networks in parallel
â”‚           â”œâ”€â”€ forward_batch_sequence(x, h_0) â†’ Batched forward pass for sequence across all networks
â”‚           â”œâ”€â”€ mutate() â†’ Apply mutations to all networks in parallel using adaptive or fixed sigma
â”‚           â”œâ”€â”€ save_hidden_states() â†’ Save current hidden states for persistence across generations/episodes
â”‚           â”œâ”€â”€ restore_hidden_states(states) â†’ Restore hidden states from previous evaluation
â”‚           â”œâ”€â”€ reset_hidden_states() â†’ Reset all hidden states to zero
â”‚           â”œâ”€â”€ get_state_dict() â†’ Get full network state for checkpointing
â”‚           â”œâ”€â”€ load_state_dict(state) â†’ Restore network state from checkpoint
â”‚           â”œâ”€â”€ get_parameters_flat() â†’ Get flattened parameter vectors for all networks
â”‚           â”œâ”€â”€ set_parameters_flat(flat_params) â†’ Set network parameters from flat vectors
â”‚           â””â”€â”€ clone_network(indices) â†’ Clone networks at specified indices to fill population
â”œâ”€â”€ ne/eval/
â”‚   â”œâ”€â”€ env.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  evaluate_env_batch(nets, env, observations, actions) â†’ Evaluate networks on batch of states from pre-recorded episodes
â”‚   â”‚   â””â”€â”€ âš™ï¸  evaluate_env_episodes(population, env, num_episodes, max_steps_per_episode, metric, state_config, curr_gen) â†’ Evaluate population on environment episodes with continual learning support
â”‚   â”œâ”€â”€ environment.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  create_env_fitness_evaluator(population, env, num_episodes, max_steps_per_episode, metric, state_config) â†’ Create fitness evaluator for environment rollouts
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  fitness_fn() â†’ Evaluate current population on environment episodes
â”‚   â”‚   â””â”€â”€ âš™ï¸  train_environment(population, train_env, test_env, num_episodes, max_steps_per_episode, metric, optimizer, max_time, eval_interval, checkpoint_path, logger, state_config) â†’ High-level environment-based training
â”‚   â”œâ”€â”€ evaluate.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  evaluate_adversarial(nets, generator_data, discriminator_data, action_size) â†’ Evaluate adversarial networks with split outputs
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  evaluate_episodes(nets, episodes, eval_fn) â†’ Evaluate networks on multiple episodes
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  evaluate_feedforward(nets, observations, actions) â†’ Evaluate feedforward networks on data using cross-entropy loss
â”‚   â”‚   â””â”€â”€ âš™ï¸  evaluate_recurrent(nets, observations, actions) â†’ Evaluate recurrent networks on sequence using cross-entropy loss
â”‚   â”œâ”€â”€ imitation.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  create_imitation_fitness_evaluators(generator_pop, discriminator_pop, target_agent, env, max_steps, hide_fn, indices, state_config, merge_mode) â†’ Create fitness evaluators for imitation learning
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  disc_fitness_fn() â†’ Evaluate discriminator fitness
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  evaluate_imitation_episode(generator_pop, discriminator_pop, target_agent, env, max_steps, hide_fn, indices, state_config, curr_gen, merge_mode) â†’ Evaluate generator and discriminator populations on imitation task
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  gen_fitness_fn() â†’ Evaluate generator fitness
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  hide_elements(obs, hide_fn, indices) â†’ Hide specific elements from observation
â”‚   â”‚   â””â”€â”€ âš™ï¸  train_imitation(generator_pop, discriminator_pop, target_agent, train_env, test_env, max_steps, hide_fn, indices, optimizer, max_time, eval_interval, checkpoint_path_gen, checkpoint_path_disc, logger, state_config, merge_mode) â†’ High-level imitation learning training
â”‚   â””â”€â”€ supervised.py
â”‚       â”œâ”€â”€ âš™ï¸  create_fitness_evaluator(population, observations, actions) â†’ Create fitness evaluator for supervised learning
â”‚       â”œâ”€â”€ âš™ï¸  fitness_fn() â†’ Evaluate current population on data
â”‚       â””â”€â”€ âš™ï¸  train_supervised(population, train_data, test_data, optimizer, max_time, eval_interval, checkpoint_path, logger) â†’ High-level supervised learning training
â”œâ”€â”€ ne/pop/
â”‚   â””â”€â”€ population.py
â”‚       â””â”€â”€ ğŸ›ï¸  Population â†’ Bridge between networks and eval/optim layers
â”‚           â”œâ”€â”€ nets() â†’ Access underlying network object
â”‚           â”œâ”€â”€ num_nets() â†’ Number of networks in population
â”‚           â”œâ”€â”€ reset_episode_tracking() â†’ Reset per-episode tracking attributes
â”‚           â”œâ”€â”€ reset_eval_tracking() â†’ Reset per-evaluation tracking attributes
â”‚           â”œâ”€â”€ reset_all_tracking() â†’ Reset all tracking attributes (including global counters)
â”‚           â”œâ”€â”€ get_actions(logits) â†’ Convert network outputs to actions
â”‚           â”œâ”€â”€ select_networks(indices) â†’ Select networks by indices and duplicate to fill population
â”‚           â”œâ”€â”€ get_parameters_flat() â†’ Get flattened parameters for all networks
â”‚           â”œâ”€â”€ set_parameters_flat(flat_params) â†’ Set parameters from flattened tensor
â”‚           â”œâ”€â”€ mutate() â†’ Apply mutations to all networks
â”‚           â”œâ”€â”€ get_state_dict() â†’ Get state for checkpointing
â”‚           â””â”€â”€ load_state_dict(state) â†’ Restore from checkpoint
â”œâ”€â”€ ne/optim/
â”‚   â”œâ”€â”€ base.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  optimize(population, fitness_fn, test_fitness_fn, selection_fn, algorithm_name, max_time, eval_interval, checkpoint_path, logger, state_config) â†’ Shared optimization loop for all evolutionary algorithms
â”‚   â”‚   â””â”€â”€ âš™ï¸  save_checkpoint(path, gen, fit_hist, test_hist, time, population, algorithm, hidden_states) â†’ Save optimization checkpoint
â”‚   â”œâ”€â”€ cmaes.py
â”‚   â”‚   â”œâ”€â”€ ğŸ›ï¸  CMAESState â†’ CMA-ES algorithm state (mean, covariance, evolution paths)
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  optimize_cmaes(population, fitness_fn, test_fitness_fn, max_time, eval_interval, checkpoint_path, logger, state_config) â†’ Optimize networks using CMA-ES
â”‚   â”‚   â””â”€â”€ âš™ï¸  select_cmaes(population, fitness) â†’ CMA-ES selection: adapt search distribution based on fitness
â”‚   â”œâ”€â”€ es.py
â”‚   â”‚   â”œâ”€â”€ âš™ï¸  optimize_es(population, fitness_fn, test_fitness_fn, max_time, eval_interval, checkpoint_path, logger, state_config) â†’ Optimize networks using Evolution Strategy
â”‚   â”‚   â””â”€â”€ âš™ï¸  select_es(population, fitness) â†’ ES selection: rank-weighted parameter averaging
â”‚   â””â”€â”€ ga.py
â”‚       â”œâ”€â”€ âš™ï¸  optimize_ga(population, fitness_fn, test_fitness_fn, max_time, eval_interval, checkpoint_path, logger, state_config) â†’ Optimize networks using Simple Genetic Algorithm
â”‚       â””â”€â”€ âš™ï¸  select_ga(population, fitness) â†’ GA selection: top 50% survive and duplicate
â””â”€â”€ root/
    â””â”€â”€ generate_structure.py
        â”œâ”€â”€ âš™ï¸  get_docstring(node) â†’ Extract first line of docstring
        â””â”€â”€ âš™ï¸  parse_file(file_path) â†’ Extract classes, methods, and functions with descriptions
```

---

**Legend:**
- ğŸ›ï¸ Class
- âš™ï¸ Function
- Methods are listed under their parent class (no icon)

**Statistics:**
- 13 directories
- 35 files
- 24 classes
- 71 methods
- 46 functions