# Batched Recurrent Networks

Batched stacked RNNs for GPU-parallel neuroevolution with all layers recurrent.

Contains BatchedRecurrent with modes: reservoir (frozen W_hh echo state style) or trainable (rank-1 u⊗v^T recurrent weights, 100 params vs 2500 for full matrix). Configured via dimensions list [input_size, layer1, ..., output_size]. Methods: forward_batch_step(x, h_states) for single timestep, forward_batch_sequence(x, h_0) for full sequence, mutate() (adaptive/fixed sigma, skips frozen W_hh for reservoir). Protocol methods (ParameterizableNetwork): get_parameters_flat(), set_parameters_flat(), clone_network(). Each layer: x → W_ih → [+ W_hh @ h_prev] → tanh → h_new. Xavier for W_ih, scaled random for W_hh/u/v. State persistence: save_hidden_states(), restore_hidden_states(), reset_hidden_states() for continual learning across generations/episodes. Implements ParameterizableNetwork protocol from protocol.py for Population and all optimizers (GA, ES, CMA-ES). get_state_dict()/load_state_dict() include hidden states for checkpointing. Optional adaptive sigma tensors if sigma_noise provided. Maintains list of hidden states per layer.
